{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import cv2 as cv\n",
    "import yaml, os\n",
    "import torch, os, yaml\n",
    "from utils.encoder import *\n",
    "from utils.llm import *\n",
    "from utils.dataset import *\n",
    "from transformers import CLIPImageProcessor, AutoProcessor\n",
    "from transformers.utils import logging\n",
    "from utils.demo_utils import *\n",
    "import numpy as np\n",
    "import os\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "from utils.dataset import get_frames\n",
    "from torchvision import transforms\n",
    "\n",
    "\n",
    "# Run settings\n",
    "run_type = f\"demo\"\n",
    "demo_config_path = f'../configs/{run_type}.yaml'\n",
    "demo_configs = yaml.safe_load(open(demo_config_path))\n",
    "device = f'cuda:{demo_configs[\"cuda\"]}'\n",
    "load_exp_path = demo_configs[\"load_exp_path\"]\n",
    "f = open(demo_configs[\"gpu_config\"])\n",
    "gpu_config = json.load(f)\n",
    "demo_path = demo_configs[\"demo_path\"]\n",
    "chat_path = demo_configs[\"chat_path\"]\n",
    "dataset = \"physiclear\" # NOTE: Assume the tactile inputs uses non-dotted GelSight Mini\n",
    "\n",
    "# RAG\n",
    "tactile_vificlip, dotted_tactile_adapter, plain_tactile_adapter, property_classifier, load_exp_configs = load_encoder(demo_configs, device)\n",
    "image_transforms = get_image_transforms(load_exp_configs[\"frame_size\"], dataset, split_name=\"test\", flip_p=0)\n",
    "if demo_configs[\"rag\"]:\n",
    "    if demo_configs[\"rag_generate_embeddings\"]:\n",
    "        print(\"\\nGenerating RAG embeddings...\")\n",
    "        generate_rag_embeddings(demo_configs, load_exp_configs, tactile_vificlip, device, demo_configs[\"rag_sample_dir\"], demo_configs[\"embedding_dir\"])\n",
    "    del dotted_tactile_adapter\n",
    "    del plain_tactile_adapter\n",
    "    del property_classifier\n",
    "    saved_embeddings, sample_tactile_paths, rag_object_ids = get_rag_embeddings(demo_configs, device)\n",
    "else:\n",
    "    tactile_vificlip = None\n",
    "    saved_embeddings = None\n",
    "    sample_tactile_paths = None\n",
    "    rag_object_ids = None\n",
    "\n",
    "# Load models\n",
    "load_exp_configs = yaml.safe_load(open(os.path.join(load_exp_path, \"run.yaml\")))\n",
    "peft = \"peft\" in demo_configs[\"load_exp_path\"]\n",
    "tokenizer_path, model_path, new_tokens, no_split_module_classes = get_model_details(load_exp_configs[\"model_type\"])\n",
    "load_exp_configs.update(demo_configs)\n",
    "start = datetime.now()\n",
    "model = load_mllm(load_exp_configs, tokenizer_path, model_path, new_tokens, no_split_module_classes, peft, device, gpu_config, exp_id=None)\n",
    "if load_exp_configs[\"use_clip\"]:\n",
    "    image_processor = CLIPImageProcessor.from_pretrained(load_exp_configs[\"use_clip\"])\n",
    "end = datetime.now()\n",
    "elapsed = (end - start).total_seconds()\n",
    "print(f\"Loaded model in {elapsed} seconds.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "inverse_transforms_list = [\n",
    "    transforms.Normalize(\n",
    "        mean=[-0.48145466/0.26862954, -0.4578275/0.26130258, -0.40821073/0.27577711],\n",
    "        std=[1/0.26862954, 1/0.26130258, 1/0.27577711]\n",
    "    ),\n",
    "]\n",
    "inverse_transforms = transforms.Compose(inverse_transforms_list)\n",
    "\n",
    "\n",
    "def plot_frames_inline(sample_path, image_transforms):\n",
    "    plt.rcParams['figure.dpi'] = 200\n",
    "    rows = 1\n",
    "    columns = 10\n",
    "    f, arr = plt.subplots(rows, columns)\n",
    "    # plt.suptitle(sample)\n",
    "    image_tensors = inverse_transforms(get_frames(sample_path, None, image_transforms, frame_size=load_exp_configs[\"frame_size\"], train=False))\n",
    "    padding_size = rows * columns - image_tensors.shape[0]\n",
    "    if padding_size > 0:\n",
    "        padding = torch.stack([image_tensors[-1]] * padding_size, dim=0)\n",
    "        image_tensors = torch.cat([image_tensors, padding], dim=0)\n",
    "    for i in range(len(image_tensors)):\n",
    "        arr[int(i%columns)].axis('off')\n",
    "        arr[int(i%columns)].imshow(image_tensors[i].cpu().numpy().transpose(1,2,0))\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "\n",
    "def get_all_embeds(user_input, prev_embeds, configs, tokenizer):\n",
    "    if \"$\" in user_input:\n",
    "        user_input = user_input.strip()\n",
    "        if \"$dr\" in user_input:\n",
    "            describe = True\n",
    "            rank = True\n",
    "        elif \"$d\" in user_input:\n",
    "            describe = True\n",
    "            rank = False\n",
    "        elif \"$r\" in user_input:\n",
    "            describe = False\n",
    "            rank = True\n",
    "        object_ids = user_input.split(\"(\")[-1].replace(\")\", \"\")\n",
    "        object_ids = [int(i.strip()) for i in object_ids.split(\",\")]\n",
    "        generation, all_embeds, question, tactile_paths_flattened = describe_rank(model, tactile_vificlip, demo_configs, load_exp_configs, object_ids, image_transforms, device, image_processor, new_tokens, saved_embeddings, sample_tactile_paths, rag_object_ids, prev_embeds, describe, rank)\n",
    "        question = question.replace(\"]\", \"[\").split(\"[\") # NOTE: Assume the tactile inputs uses the non-dotted GelSight Mini\n",
    "        print(f\"###### USER: {question[0]}\\n\", flush=True)\n",
    "        tactile_count = 0\n",
    "        for chunk in question[1:]:\n",
    "            if \"frames\" in chunk:\n",
    "                plot_frames_inline(tactile_paths_flattened[tactile_count], image_transforms)\n",
    "                tactile_count += 1\n",
    "            else:\n",
    "                print(chunk, flush=True)\n",
    "    else:\n",
    "        print(f\"###### USER: {user_input}\\n\")\n",
    "        messages = [\n",
    "            {\"role\": \"user\", \"content\": user_input}\n",
    "        ]\n",
    "        question_template = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "        question_embeds = process_user_input(question_template, image_processor, model, tokenizer, device, new_tokens, configs[\"frame_size\"], image_transforms)\n",
    "        generation, generation_embeds, question_embeds = generate(question_embeds, model, demo_configs[\"max_new_tokens\"], prev_embeds=prev_embeds)\n",
    "        all_embeds = torch.cat([question_embeds, generation_embeds], dim=1)\n",
    "    return all_embeds, generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "\n",
    "with torch.no_grad():\n",
    "    user_input = input(f\"USER: \")\n",
    "    while user_input == \"restart\":\n",
    "        clear_output(wait=True)\n",
    "        user_input = input(f\"USER: \")\n",
    "    all_embeds = None\n",
    "    all_embeds, generation = get_all_embeds(user_input, all_embeds, load_exp_configs, model.tokenizer)\n",
    "    print(f\"###### ASSISTANT: {generation}\\n\", flush=True)\n",
    "    user_input = input(\"USER: \")\n",
    "\n",
    "    while user_input.strip() != \"exit\":\n",
    "        if user_input == \"restart\":\n",
    "            clear_output(wait=True)\n",
    "            all_embeds = None\n",
    "            user_input = input(f\"USER: \")\n",
    "        elif len(user_input) == 0:\n",
    "            user_input = input(f\"USER: \")\n",
    "        else:\n",
    "            all_embeds, generation = get_all_embeds(user_input, all_embeds, load_exp_configs, model.tokenizer)\n",
    "            print(f\"###### ASSISTANT: {generation}\\n\", flush=True)\n",
    "            user_input = input(\"USER: \")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "octopis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
